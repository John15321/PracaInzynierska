\section{Sieć neuronowa}

Sieć neuronowa \cite{NeuralNetwork} \cite{ArtificialNeuralNetwork} \cite{DeepLearningFromScratch} \cite{HandsOnMachineLearning} jest siecią bądź układem neuronów, w nowoczesnym zrozumieniu pojęcie jest stosowane głównie w odniesieniu do sztucznych sieci neuronowych , które składają się ze sztucznych komputerowo symulowanych, uproszczonych modeli neuronów. Sztuczne sieci neuronowe są inspirowane przez biologiczne sieci neuronowe, które tworzą mózgi zwierząt.
Każde połączenie, czyli inaczej każda synapsa w biologicznej sieci neuronowej może transmitować sygnał do innych neuronów. Sztuczny neuron otrzymuje sygnał, następnie przetwarza go i może zasygnalizować inne połączone z nim neurony. Sam sygnał jest liczbą rzeczywistą, wyjście każdego neuronu obliczane za pomocą jakiejś nie liniowej sumy wejścia. Połączenia neuronów posiadają wagę, która jest dopasowywana podczas procesu uczenia. Waga danego połączenia zwiększa lub pomniejsza siłę sygnału w danym połączeniu. Neurony mogą posiadać granice wartości takiego sygnału, taką, że dany sygnał zostanie przepuszczony jeśli zsumowany sygnał przekracza jakąś wartość. Zazwyczaj neurony są agregowane w tak zwane warstwy. Różne warstwy mogą powodować różne transformacje sygnału bazując na ich wejściach. Sygnał podróżuje od warstwy wejściowej, do ostatniej warstwy, która jest warstwą wejściową, zwykle po przejściu kilku wewnętrznych warstw.


\begin{figure}[h]
    \centering
    \smallimage{img/Colored_neural_network.svg.png}
    \caption{Symboliczny model sieci neuronowej \cite{ColoredNeuralNetwork}}
    \label{img:nn_symbolic_diagram}
\end{figure}


\clearpage


W 1943 roku Warren McCulloch i Walter Pitts zapoczątkowali dziedzinę poprzez stworzenie stworzenie obliczeniowego modelu sieci neuronowej. W późnych latach 40, dwudziestego wieku, D. O. Hebb stworzył hipotezę bazującą na mechanizmie neuroplastyczności \cite{Neuroplastycznosc}, czyli zdolności tkanki nerwowej do tworzenia nowych połączeń mających na celu jej reorganizację, adaptację lub zmianę. Jest to powszechna cecha neuronów, która występuje na każdym poziomie układu nerwowego. Teoria Hebbiana postuluje zwiększenie wydajności synaps na skutek ciągłej stymulacji. W roku 1945 Farley i Wesley A. Clark jako pierwsi użyli maszyny obliczeniowej (wtedy nazywanej jeszcze "kalkulatorami"), do symulacji sieci neuronów Hebbiana. Pierwsze funkcjonalne sieci neuronowe z wieloma warstwami zostały opublikowane przez Ivakhnenko i Lape w roku 1965. A podstawy algorytmu propagacji wstecznej (z ang. backpropagation).

W uczeniu maszynowym propagacja wsteczna \cite{Backpropagation} jest szeroko używanym algorytmem służącym do trenowania sieci neuronowych. Przy szkoleniu sieci neuronowej propagacja wsteczna oblicza gradient funkcji błędu względem wagi sieci, dla konkretnego wejścia/wyjścia, i robi to efektywnie pod względem obliczeniowym w przeciwieństwie do naiwnych metod obliczeniowych obliczania gradientu względem każdej wagi indywidualnie. Efektywność sprawia, że możliwe jest użycie metod gradientowych do trenowania sieci wielowarstwowych, uaktualniania wag, aby zminimalizować funkcję błędu. Metoda gradientu prostego, lub jego różne warianty takie jak metoda gradientu stochastycznego są bardzo często używane. Algorytm propagacji wstecznej działa na zasadzie obliczania gradientu funkcji błędu każdej z wagi korzystając z reguły łańcuchowej, obliczając gradient każdej warstwy po kolei, iterując w tył od ostatniej warstwy, aby uniknąć powtarzających się obliczeń, co między innymi jest przykładem programowania dynamicznego.

Początek sztucznych sieci neuronowych był spowodowany inspiracją i próbą użycia architektury ludzkiego mózgu tak, aby mógł on wykonywać zadanie, które są bardzo problematyczne, dla konwencjonalnych algorytmów. Dość szybko nastąpiła reorientacja ku ulepszeniu empirycznych wyników, opuszczając tym samym próby prawdziwego, zgodnego z ich biologicznymi prekursorami. Neurony połączone są ze sobą różnorakimi metodami tak, aby umożliwić wyjście niektórych neuronów, aby stało się wejściem innych. W ten sposób sieć tworzy ukierunkowany graf wag.

Sztuczna sieć neuronowa składa się z kolekcji symulowanych neuronów. Każdy neuron jest wierzchołkiem w grafie, który jest połączony z innymi wierzchołkami za pomocą połączeń nazywanych krawędziami. Każda krawędź ma wagę, która determinuje siłę z jaką oddziaływuje na inne neurony.



\section{Uczenie przez wzmacnianie}

Uczenie przez wzmacnianie (ang. reinforcement learning, RL) \cite{ReinforcementLearning} \cite{ReinforcementLearningBook} \cite{{DeepReinforcementLearning}} jest jednym z typów uczenia maszynowego, które koncentruje się na inteligentnych agentach, które dokonują akcji w danych środowiskach tak, aby zmaksymalizować kumulacyjną nagrodę. Reinforcement learning jest jednym z trzech podstawowych paradygmatów uczenia maszynowego, obok uczenia nadzorowanego (Supervised Learning), oraz uczenia nienadzorowanego (Unsupervised Learning).

Reinforcement learning różni się od uczenia nadzorowanego tym, że nie potrzebuje poukładanego, oznakowanego zestawu danych wejść i wyjść, które mają zostać zaprezentowane sieci. Dodatkowo sub-optymalne zachowania i rozwiązania sieci nie wymagają korekcji. Zamiast tego trening sieci skupia się na balansie pomiędzy eksploracją nowych zachowań, oraz eksploatacją już istniejących umiejętności.
Interakcja pomiędzy siecią, a środowiskiem przebiega w następujący sposób, gdzie to agent umieszczony w danym środowisku sczytuje jego stan. Stan środowiska jest wejściem sieci neuronowej. Następnie na podstawie danego stanu podejmuje decyzję, która jest wskazana przez wyjście sieci neuronowej. Podjęta decyzja wpływa na środowisko, następnie na podstawie rezultatu uzyskanego poprzez daną akcję dokonywana jest poprawa wag połączeń sieci. Poniższy diagram przedstawia uproszczony schemat tego działania, biorąc za środowisko grę komputerową:

\begin{figure}[h]
    \centering
    \bigimage{img/GameAI_Diagram.png}
    \caption{Symboliczny model sieci w połączeniu z grą}
    \label{img:rf_learning_diagram}
\end{figure}


\section{Q-learning}

Q-learning \cite{HandsOnQLearningWithPython} \cite{Qlearning} jest to bezmodelowy algorytm nauczania przez wzmacnianie, którego zadaniem jest nauczenie się wartości danej akcji w danym środowisku i stanie. Nie wymaga modelu środowiska. Ważną wartością w tej metodzie nauczania jest wartość Q, która oznacza jakość wykonanej akcji.
Implementacja Q-learningu zakłada następującą procedurę:

\begin{enumerate}
    \centering
    \item Inicjalizacja wartości Q
    \item Wybranie akcji przez model
    \item Wykonanie akcji
    \item Pomiar nagrody
    \item Zaktualizowanie wartości Q + trening modelu
    \item Powrót do punktu numer 1.
\end{enumerate}

W pierwszym kroku inicjalizujemy wartość Q, poprzez inicjalizację naszego modelu losowymi parametrami. Następnie w drugim kroku wybieramy akcję, którą ma wykonać nasz agent. Akcja jest wybierana na podstawie predykcji modelu wykonując metodę \lcode{model.predict(state)}, dodatkowo co jakiś arbitralny procent ruchów decydujemy się na losowy ruch w celu umożliwienia modelowi eksploracji innych możliwości niż te już potencjalnie wyuczone, co jest głównie przydatne na początku nauki modelu, kiedy nie ma on doświadczenia ze środowiskiem. Kolejnym krokiem jest wykonanie danej akcji na środowisku i zmierzenie nagrody, która została spowodowana danym ruchem, dzięki tej zmierzonej wartości możliwe jest dokonanie aktualizacji wartości Q, która jest obiektywnym miernikiem jakości akcji, dzięki czemu możliwe jest trenowanie neuronów na danej akcji.

Funkcją błędu odpowiedzialną za obliczenie wartości Q jest równanie Bellman'a \cite{BellmanEquation}, gdzie nowa wartość Q jest obliczana z sumy starej wartości Q z współczynnikiem uczenia pomnożonym przez sumę nagrody za dokonaną akcję, wraz z maksymalną spodziewaną przyszłościową wartością Q biorąc pod uwagę nowy stan przemnożoną przez discount-rate minus obecna wartość Q:


\begin{figure}[h]
    \centering
    \bigimage{img/bellman_eq.png}
    \caption{Równanie Bellman'a \cite{BellmanEquationPic}}
    \label{img:bellman_eq}
\end{figure}

Dzięki równaniu Bellman'a możliwe jest skonstruowanie nowej funkcji błędu średniokwadratowego \cite{BladSredniokwadratowy} opartego o wartość Q:

$$
    Błąd = (Q_{nowe} - Q)^{2}
$$

\clearpage

\section{Implementacja}


Na początku definiowane jest kilka podstawowych parametrów takich jak maksymalna liczba trzymanych obiektów w pamięci, batch-size \cite{BatchNormalization} (czyli liczbę egzemplarzy treningowych na których będzie szkolona sieć naraz), oraz współczynnik uczenia, czyli parametr wiążący funkcję błędu ze zmianą wag:

\begin{onepage}
    \begin{lstlisting}[
    caption={Podstawowe parametry},
    label=code:basic_params,
    language={Python},
    numbers=none
]
MAX_MEMORY = 100000
BATCH_SIZE = 1000
LEARNING_RATE = 0.001
    \end{lstlisting}
\end{onepage}


Kolejnym krokiem jest definicja klasy \lcode{Agent}, która definiuje parametry takie jak rozmiar warstw wejściowych, wewnętrznych/ukrytych, oraz wyjściowych sieci neuronowej, współczynnik $\varepsilon$ kontrolujący losowość akcji agenta, oraz współczynnik gamma, inaczej zwany jako discount-rate. Dodatkowo tworzymy model sieci neuronowej \lcode{self.model} korzystając z klasy \lcode{LinearQNet} z biblioteki PyTorch, oraz \lcode{self.trainer}, który jest obiektem odpowiedzialnym za trening sieci neuronowej, zgodnie z podanymi parametrami:

\begin{onepage}
    \begin{lstlisting}[
    caption={Definicja klasy \lcode{Agent}},
    label=code:agent_class_definition,
    language={Python},
    numbers=none
]
class Agent:
    def __init__(
        self,
        input_size: int,
        output_size: int,
        epsilon: float = 0,
        gamma: float = 0.9,
        hidden_size: int = 256,
    ) -> None:
        self.number_of_games = 0
        self.epsilon = epsilon  # Controls randomness
        self.gamma = gamma  # Discount rate
        self.memory = deque(maxlen=MAX_MEMORY)
        self.model = LinearQNet(input_size, hidden_size, output_size)
        self.trainer = QTrainer(
            self.model, learning_rate=LEARNING_RATE, gamma=self.gamma
        )
    \end{lstlisting}
\end{onepage}

Utworzona klasa posiada metodę \lcode{remember()}, służącą do dodania stanu, akcji, nagrody, stanu będącego rezultatem akcji, oraz stanu gry, do specjalnej listy, która je zachowuje w celu przeprowadzenia treningu sieci neuronowej:


\begin{onepage}
    \begin{lstlisting}[
    caption={Definicja metody \lcode{remember}},
    label=coderemember_method:,
    language={Python},
    numbers=none
]
def remember(self, state, action, reward, next_state, game_state):
    self.memory.append((state, action, reward, next_state, game_state))
    \end{lstlisting}
\end{onepage}



Następnie definiowane są metody nauki krótko pamięciowej, długo pamięciowej, które wzywają metodę \lcode{self.trainer.train_step()} klasy \lcode{QTrainer}:


\begin{onepage}
    \begin{lstlisting}[
    caption={Metoda \lcode{train_long_memory}},
    label=code:long_memory,
    language={Python},
    numbers=none
]
def train_long_memory(self):
    if len(self.memory) > BATCH_SIZE:
        mini_sample: List[Tuple] = random.sample(self.memory, BATCH_SIZE)
    else:
        mini_sample: List[Tuple] = self.memory

    states, actions, rewards, next_states, game_states = zip(*mini_sample)
    self.trainer.train_step(states, actions, rewards, next_states, game_states)

\end{lstlisting}
\end{onepage}

\begin{onepage}
    \begin{lstlisting}[
    caption={Metoda \lcode{train_short_memory}},
    label=code:short_memory,
    language={Python},
    numbers=none
]
def train_short_memory(self, state, action, reward, next_state, game_state):
    self.trainer.train_step(state, action, reward, next_state, game_state)

    \end{lstlisting}
\end{onepage}


%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{onepage}
    \begin{lstlisting}[
    caption={},
    label=code:,
    language={Python},
    numbers=none
]

    \end{lstlisting}
\end{onepage}
